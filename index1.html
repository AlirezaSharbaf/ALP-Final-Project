<!DOCTYPE html>
<html lang="en-US">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NLP Class Project | Fall 2024 CSCI 5541 | University of Minnesota</title>
  <link rel="stylesheet" href="./files/bulma.min.css" />
  <link rel="stylesheet" href="./files/styles.css">
  <link rel="preconnect" href="https://fonts.gstatic.com/">
  <link href="./files/css2" rel="stylesheet">
  <link href="./files/css" rel="stylesheet">
  <base href="." target="_blank">
</head>
<body>
  <div class="wrapper">
    <h1 style="font-family: 'Lato', sans-serif;">Robustness of NLP Models Against Adversarial Attacks</h1>
    <h4 style="font-family: 'Lato', sans-serif;">Fall 2024 CSCI 5541 NLP: Class Project - University of Minnesota</h4>
    <h4 style="font-family: 'Lato', sans-serif;">Team AdversAI</h4>

    <div class="authors-wrapper">
      <div class="author-container">
        <div class="author-image">
          <img src="">
        </div>
        <p>Mobina</p>
      </div>
      <div class="author-container">
        <div class="author-image">
          <img src="">
        </div>
        <p>Member 2</p>
      </div>
      <div class="author-container">
        <div class="author-image">
          <img src="">
        </div>
        <p>Member 3</p>
      </div>
      <div class="author-container">
        <div class="author-image">
          <img src="">
        </div>
        <p>Member 4</p>
      </div>
    </div>

    <br/>

    <div class="authors-wrapper">
      <div class="publication-links">
        <span class="link-block">
          <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark is-outlined">
            <span>Final Report</span>
          </a>
        </span>
        <span class="link-block">
          <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark is-outlined">
            <span>Code</span>
          </a>
        </span>
        <span class="link-block">
          <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark is-outlined">
            <span>Model Weights</span>
          </a>
        </span>
      </div>
    </div>
  </div>

  <div class="wrapper">
    <hr>
    <h2 id="abstract">Abstract</h2>
    <p>Our project investigates the robustness of NLP models against adversarial attacks. We fine-tune the BERT model on both clean and adversarial data (generated via synonym-based perturbations) to understand its resilience. Despite adversarial training, accuracy improvements were modest, highlighting the challenges of making models robust to diverse adversarial inputs.</p>
    <hr>

    <h2 id="teaser">Teaser Figure</h2>
    <p>A figure that conveys the main idea behind the project or the main application being addressed. This figure is from <a href="https://arxiv.org/pdf/2210.07469">StyLEx</a>.</p>
    <p class="sys-img"><img src="./files/teaser.png" alt="Teaser Figure"></p>
    <h3 id="the-timeline-and-the-highlights">Any subsection</h3>
    <p>Explanation about the figure goes here.</p>
    <hr>

    <h2 id="introduction">Introduction / Background / Motivation</h2>
    <p><b>What did you try to do? What problem did you try to solve? Articulate your objectives using absolutely no jargon.</b></p>
    <p>We aimed to enhance the robustness of NLP models by making them more resistant to adversarial attacks that change the input text while preserving its meaning.</p>

    <p><b>How is it done today, and what are the limits of current practice?</b></p>
    <p>Current models are trained on clean data, which leaves them vulnerable to adversarial perturbations. Adversarial training can help, but it is limited by the diversity and unpredictability of attacks.</p>

    <p><b>Who cares? If you are successful, what difference will it make?</b></p>
    <p>Improving robustness makes NLP models more reliable in real-world applications, enhancing security and reducing susceptibility to adversarial manipulations.</p>
    <hr>

    <h2 id="approach">Approach</h2>
    <p><b>What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?</b></p>
    <p>We fine-tuned BERT on adversarial examples generated by changing words to their synonyms. This aimed to expose the model to variations in inputs and improve its resilience. We believed that by expanding the training data with adversarial examples, the model would learn to generalize better under attacks.</p>

    <p><b>What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?</b></p>
    <p>We anticipated that replacing too many words could compromise the integrity of the input sentence. Indeed, the model showed limited improvement, likely due to the variability of possible adversaries and the challenge in effectively generalizing from such attacks.</p>
    <hr>

    <h2 id="results">Results</h2>
    <p><b>How did you measure success? What experiments were used? What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why?</b></p>
    <p>We measured success by evaluating the model's accuracy on both clean and adversarial datasets. Before adversarial training, BERT achieved 88.09% accuracy on adversarial examples. After adversarial training, the accuracy improved by 1.05%. On the clean dataset, the model performed at 92.83% accuracy. This limited improvement suggests that the model struggles to generalize effectively to diverse adversarial inputs.</p>
    <table>
      <thead>
        <tr>
          <th style="text-align: center"><strong>Experiment</strong></th>
          <th style="text-align: center">1</th>
          <th style="text-align: center">2</th>
          <th style="text-align: center">3</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td style="text-align: center"><strong>Sentence</strong></td>
          <td style="text-align: center">Example 1</td>
          <td style="text-align: center">Example 2</td>
          <td style="text-align: center">Example 3</td>
        </tr>
        <tr>
          <td style="text-align: center"><strong>Errors</strong></td>
          <td style="text-align: center">error A, error B, error C</td>
          <td style="text-align: center">error C</td>
          <td style="text-align: center">error B</td>
        </tr>
      </tbody>
      <caption>Table 1. Examples of adversarial errors.</caption>
    </table>
    <div style="text-align: center;">
      <img style="height: 300px;" alt="Results" src="./files/results.png">
    </div>
    <hr>

    <h2 id="conclusion">Conclusion and Future Work</h2>
    <p>Our work has demonstrated the challenges of adversarial training in NLP, particularly when faced with diverse synonym-based attacks. In future work, we plan to evaluate the robustness of Meta's newly introduced quantized Llama models against adversarial inputs, considering the trade-offs between efficiency gains and robustness. These quantized models, while offering significant speedup and size reduction, may present new vulnerabilities that we aim to investigate thoroughly.</p>
    <hr>
  </div>
</body>
</html>
